\section{Introduction}

Full multigrid (FMG) is a provably asymptotically exact, non-iterative algebraic equation solver for discretized
elliptic partial differential equations with work complexity of about five residual calculations, or what is known as
textbook multigrid efficiency (TME), for the constant coefficient Laplacian~\cite{BrandtDiskin1994}. For some classes of
elliptic problems, TME can be shown analytically, but it has been observed experimentally for a very broad spectrum of
problems~\cite{ThomasDiskinBrandt2001,trottenberg2001multigrid,Adams-10a}. In addition, the Full Approximation Scheme
(FAS) multigrid is applicable to general nonlinear elliptic equations.

Multigrid methods are widely used in practice, but distributed memory parallel multigrid presents many implementation
challenges. The benefits for future computer architectures of the reduction in communication, measured in both power and
time, are well-documented in~\cite{AdamsBrownKnepleySamtaney2016}. Segmental refinement explicitly decouples subdomain
processing on the finest multigrid levels, which improves data locality, amortizes latency costs, and reduces data
dependencies. However, large surveys of the scientific computing userbase, such as~\cite{PrabhuEtAl2011}, show that a
majority of practitioners are confined to a desktop computer. In addition, those users with parallel code have very long
running jobs (${} > 50$\% run for more than 24 hours), and make very little attempt to optimize the code, leading to the
conclusion that the leading reason for parallelism is to run jobs too large for a single machine. Our limited memory
implementation of SRMG can allow practitioners to run very large potential problems ($10^{12}$ unknowns) in the same 24
hour timeframe on a standard desktop machine. This capability has the potential to revolutionize scientific computing
for the vast mass of users.

Possible applications

\section{Background}

Beginning in the 1970s, Brandt~\cite{Brandt77,DinarThesis1979,brandt1984} proposed segmental refinement multigrid (SRMG)
as a polylogarithmic memory alternative to traditional FMG that does not store the entire solution in memory at any one
time. More recent work~\cite{BrandtDiskin1994,MohrRude1998,Mohr2000,AdamsBrownKnepleySamtaney2016} has concentrated on
the benefits for distributed memory computing, namely that SRMG has no interprocess communication on the finest
grids. This paper presents an implementation with polylogarithmic memory requirements, and experimentally examines the
accuracy, memory scalability, and dependence on interpolation order. We present multilevel numerical results,
complementing~\cite{AdamsBrownKnepleySamtaney2016}, and verify complexity models for both computation and memory.

In prior work, it was unclear whether an actual implementation could scale indefinitely with a finite buffer size. In
fact, contrary to the theory sketched in~\cite{BrandtDiskin1994}, in~\cite{AdamsBrownKnepleySamtaney2016} it appears
that the buffer size must grow with the number of levels. Second, no prior implementation had demonstrated
polylogarithmic memory usage. We address both of these questions in Section~\ref{sec:results}.

\section{Methodology}

We follow~\cite{BrandtDiskin1994} in employing a vertex-centered discretization for the multigrid iteration, in this
case a 5-point finite difference stencil on a Cartesian grid, embodied in the long-standing PETSc example code, SNES
ex5~\cite{petsc-user-ref}. This code, in fact, solves the Bratu equation, but we reduce this to the Laplace equation by
setting the parameter $\lambda = 0$.

\subsection{Low Memory Functionals}

Since our implementation is low-memory by design, we must make use of the fine grid solution one patch at a
time. Fortunately, we can do this by computing functionals $\mathcal{F}$ of the solution which can be expressed as
integrals over the domain,
\begin{align}
  \mathcal{F}(u) = \int_\Omega f(u)\,dx.
\end{align}
Since the integral is additive, we can express this as a sum over patches
\begin{align}
  \mathcal{F}(u) = \sum_P \int_{\Omega_P} f(u)\,dx,
\end{align}
and we can batch this processing by computing several functionals at once on each patch. For example, we use the error
functional $\mathcal{E}^p$ to check that our manufactured solution converges at the correct rate,
\begin{align}
  \mathcal{E}^p(u) = \left( \sum_P \int_{\Omega_P} |u - u^*|^p\,dx \right)^{1/p},
\end{align}
so we integrate $|u - u^*|$, $|u - u^*|^2$, and $\max |u - u^*|$ simultaneously on each patch, keeping the result in a
separate accumulator, and then after all patches at a given level are processed, we take the appropriate root and output
the result. Thus, we need two functions for each user functional, one that computes the integral over a patch, and one
that does final processing and outputs the result.

\subsection{Higher Order Interpolation}

Rujeko

\subsection{Computational and Memory Models}

Jeremy

Let $n_0$ be the number of unknowns in one dimension at the coarsest level. Consider at i levels of refinement, we have $2^i n_0$ unknowns in one dimension as a result of i smoothing operations. Denote the  number of unknowns at the smoothest level by $N$.  Thus the maximum number of refinements is $l = log(N) - log(n_0)$. Since we half (in each dimension) the size of the subdomains domain every time , we have $\frac{2^i}{2^i} n_0 = n_0$ unknowns in the area of interest at every level, but must also calculate cells in a buffer region. 

Note that the number of cells in the buffer region grows at a constant rate (which can be proven). So we need $n_0 + c i$ cells in each dimension at i levels of refinement. Larger buffer regions correspond to a larger constant. 

Let d be the dimension of the domain. 

$O(\sum_{i=0}^l (n_0 + c i)^d) = O(n_0 * l) + O(\sum_{i=0}^l c i^d) = O(n_0 logN) + O(l^{d+1}) = O( n_0 log N) + O(logN)^d $
\subsection{Implementation}

Frankie: documentation

Derek: build/test

\section{Results}\label{sec:results}

List all plots and the reason for each plot.

Michael: Mesh Convergence MMS 1, 2, 3, 4 (Correctness)

Ryan: Patch Convergence MMS 2, 3, 4 (Accuracy)

Jeremy: Memory Usage (Efficiency)

Ryan: Patch Convergence with different buffer size (Accuracy)

John: Patch Convergence with different interpolation order (Accuracy)

Shoeb: Runtime vs N (Efficiency)

\section{Conclusions}

Eric: Restate main result

future work, open problems
